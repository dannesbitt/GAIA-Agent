{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX9bk6ASkencjLuJLmeOc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dannesbitt/GAIA-Agent/blob/main/GAIA_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langgraph langchain_openai langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq8_eyzKtl8t",
        "outputId": "01e41fcf-4921-4da2-cce8-e2956aba0432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZgLOVDusN0Q",
        "outputId": "cf708173-3a63-4c78-a538-8c4acbf0208a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: The country with the least number of athletes at the 1928 Summer Olympics was Luxembourg, which had only 3 athletes. The IOC country code for Luxembourg is **LUX**.\n",
            "Final State: {'messages': [{'role': 'user', 'content': \"Question: What country had the least number of athletes at the 1928 Summer Olympics? If there's a tie for a number of athletes, return the first in alphabetical order. Give the IOC country code as your answer.\\n\\n\"}, ChatCompletionMessage(content='The country with the least number of athletes at the 1928 Summer Olympics was Luxembourg, which had only 3 athletes. The IOC country code for Luxembourg is **LUX**.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None)], 'tool_calls': [], 'final_response': 'The country with the least number of athletes at the 1928 Summer Olympics was Luxembourg, which had only 3 athletes. The IOC country code for Luxembourg is **LUX**.', 'needs_tool_call': False}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from typing import TypedDict, List\n",
        "from openai import OpenAI\n",
        "from langgraph.graph import Graph, END\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up OpenAI client (ensure OPENAI_API_KEY is set in your environment)\n",
        "OPEN_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=OPEN_API_KEY)\n",
        "\n",
        "# Define the state structure\n",
        "class State(TypedDict):\n",
        "    messages: List[dict]\n",
        "    tool_calls: List[dict]\n",
        "    final_response: str\n",
        "    needs_tool_call: bool\n",
        "\n",
        "# Define available tools (optional, included for flexibility)\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {},\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Dummy tool execution function (replace with actual tools if needed)\n",
        "def execute_tool(tool_call):\n",
        "    if tool_call[\"function\"][\"name\"] == \"get_weather\":\n",
        "        return \"It's sunny today.\"\n",
        "    return \"Tool not found.\"\n",
        "\n",
        "# Helper function to fetch task files\n",
        "def fetch_task_files(task_id):\n",
        "    \"\"\"Fetches files associated with the task_id from the API.\"\"\"\n",
        "    url = f\"https://agents-course-unit4-scoring.hf.space/files/{task_id}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        files_data = response.json()\n",
        "        # Assuming files_data is a list of {\"filename\": \"...\", \"content\": \"...\"}\n",
        "        file_contents = {file[\"filename\"]: file[\"content\"] for file in files_data}\n",
        "        return file_contents\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching files for task_id {task_id}: {e}\")\n",
        "        return {}\n",
        "\n",
        "# Define the nodes\n",
        "def input_node(state: State) -> State:\n",
        "    \"\"\"Fetches a question from the API, checks for file_id, downloads files if present, and constructs the initial user message.\"\"\"\n",
        "    if not state['messages']:\n",
        "        try:\n",
        "            response = requests.get('https://agents-course-unit4-scoring.hf.space/random-question')\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            question = data['question']\n",
        "            file_id = data.get('file_id', None)  # Check for 'file_id' in the response\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching question: {e}\")\n",
        "            question = \"What is the meaning of life?\"\n",
        "            file_id = None\n",
        "\n",
        "        # Construct the user message\n",
        "        user_message = f\"Question: {question}\\n\\n\"\n",
        "        if file_id:\n",
        "            file_contents = fetch_task_files(file_id)\n",
        "            if file_contents:\n",
        "                user_message += \"File contents:\\n\"\n",
        "                for filename, content in file_contents.items():\n",
        "                    user_message += f\"{filename}:\\n{content}\\n\\n\"\n",
        "\n",
        "        state['messages'].append({\"role\": \"user\", \"content\": user_message})\n",
        "    state['needs_tool_call'] = False\n",
        "    return state\n",
        "\n",
        "def llm_node(state: State) -> State:\n",
        "    \"\"\"Calls the OpenAI LLM with the current messages and processes the response.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=state['messages'],\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "    assistant_message = response.choices[0].message\n",
        "    state['messages'].append(assistant_message)\n",
        "\n",
        "    if assistant_message.tool_calls:\n",
        "        state['needs_tool_call'] = True\n",
        "    else:\n",
        "        state['final_response'] = assistant_message.content\n",
        "        state['needs_tool_call'] = False\n",
        "    return state\n",
        "\n",
        "def tool_node(state: State) -> State:\n",
        "    \"\"\"Executes tool calls and appends results to messages.\"\"\"\n",
        "    assistant_message = state['messages'][-1]\n",
        "    for tool_call in assistant_message.tool_calls:\n",
        "        result = execute_tool(tool_call)\n",
        "        state['messages'].append({\n",
        "            \"role\": \"tool\",\n",
        "            \"content\": result,\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "        })\n",
        "    return state\n",
        "\n",
        "def output_node(state: State) -> State:\n",
        "    \"\"\"Prints the LLM's response.\"\"\"\n",
        "    print(\"Response:\", state['final_response'])\n",
        "    return state\n",
        "\n",
        "# Create the graph\n",
        "graph = Graph()\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"input\", input_node)\n",
        "graph.add_node(\"llm\", llm_node)\n",
        "graph.add_node(\"tool\", tool_node)\n",
        "graph.add_node(\"output\", output_node)\n",
        "\n",
        "# Define edges\n",
        "graph.add_edge(\"input\", \"llm\")\n",
        "graph.add_conditional_edges(\n",
        "    \"llm\",\n",
        "    lambda state: \"tool\" if state['needs_tool_call'] else \"output\",\n",
        "    {\"tool\": \"tool\", \"output\": \"output\"}\n",
        ")\n",
        "graph.add_edge(\"tool\", \"input\")\n",
        "graph.add_edge(\"output\", END)\n",
        "\n",
        "# Set entry point\n",
        "graph.set_entry_point(\"input\")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "# Run the workflow\n",
        "initial_state = {\n",
        "    \"messages\": [],\n",
        "    \"tool_calls\": [],\n",
        "    \"final_response\": \"\",\n",
        "    \"needs_tool_call\": False\n",
        "}\n",
        "result = app.invoke(initial_state)\n",
        "print(\"Final State:\", result)\n"
      ]
    }
  ]
}